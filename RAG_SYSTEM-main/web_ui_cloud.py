"""
RAG System - Cloud Web UI (Streamlit Cloud Compatible)
Uses HuggingFace Inference API instead of Ollama
"""

import sys
sys.path.insert(0, 'src')

import streamlit as st
from pathlib import Path
import time
import requests
import os

# Page configuration
st.set_page_config(
    page_title="RAG System",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        text-align: center;
        color: rgba(0, 0, 0, 0.6);
        margin-bottom: 2rem;
    }
    @media (prefers-color-scheme: dark) {
        .sub-header {
            color: rgba(255, 255, 255, 0.6);
        }
    }
    .stTextInput > div > div > input {
        font-size: 1.1rem;
    }
    .source-box {
        background: rgba(30, 136, 229, 0.05);
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border: 1px solid rgba(30, 136, 229, 0.2);
        border-left: 4px solid #1E88E5;
    }
    .answer-box {
        background: rgba(30, 136, 229, 0.08);
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        font-size: 1.1rem;
        line-height: 1.6;
        border: 1px solid rgba(30, 136, 229, 0.15);
    }
    .stat-card {
        background: rgba(30, 136, 229, 0.05);
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
        border: 2px solid rgba(30, 136, 229, 0.3);
    }
    .stat-number {
        font-size: 2rem;
        font-weight: bold;
        color: #1E88E5;
    }
    .stat-label {
        color: rgba(0, 0, 0, 0.6);
        font-size: 0.9rem;
    }
    @media (prefers-color-scheme: dark) {
        .stat-label {
            color: rgba(255, 255, 255, 0.6);
        }
    }
    .footer-text {
        text-align: center;
        color: rgba(0, 0, 0, 0.5);
        font-size: 0.9rem;
    }
    @media (prefers-color-scheme: dark) {
        .footer-text {
            color: rgba(255, 255, 255, 0.5);
        }
    }
</style>
""", unsafe_allow_html=True)

# Initialize ChromaDB and Embeddings
@st.cache_resource
def initialize_system():
    """Initialize the RAG system components"""
    from sentence_transformers import SentenceTransformer
    import chromadb
    
    # Initialize embedding model
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    # Initialize ChromaDB
    client = chromadb.PersistentClient(path="./chroma_db")
    collection = client.get_or_create_collection(
        name="rag_documents",
        metadata={"hnsw:space": "cosine"}
    )
    
    return model, collection

def generate_answer_from_context(question, context_docs):
    """Generate answer directly from retrieved documents (no LLM needed!)"""
    if not context_docs:
        return "I couldn't find any relevant information in your documents to answer this question."
    
    # Combine all retrieved context
    full_context = "\n\n".join(context_docs)
    
    # Simple extractive answer: return most relevant chunks
    answer = f"""Based on your documents, here's what I found:

üìÑ **Relevant Information:**

{full_context[:1500]}

---

üí° **Note:** This is a direct extract from your documents. For AI-generated summaries and more natural answers, add a HuggingFace token (FREE): https://huggingface.co/settings/tokens"""
    
    return answer

def query_huggingface(prompt, hf_token=None):
    """Query HuggingFace API with multiple model fallbacks"""
    
    # If no token, return None to trigger local answer generation
    if not hf_token or hf_token == "":
        return None
    
    # Try multiple models in order of preference
    models = [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "mistralai/Mistral-7B-Instruct-v0.3",
        "microsoft/Phi-3-mini-4k-instruct",
        "google/flan-t5-large"
    ]
    
    headers = {"Authorization": f"Bearer {hf_token}"}
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 500,
            "temperature": 0.7,
            "top_p": 0.95,
            "return_full_text": False
        }
    }
    
    for model in models:
        try:
            API_URL = f"https://api-inference.huggingface.co/models/{model}"
            response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    return result[0].get('generated_text', 'No response generated')
                return str(result)
            elif response.status_code == 503:
                # Model is loading, try next one
                continue
            elif response.status_code != 410:
                # Not a "gone" error, might be rate limit, try next
                continue
        except Exception:
            continue
    
    # If all models fail, provide helpful message
    return """‚ö†Ô∏è AI models are currently unavailable. To get full AI-generated answers:

üìå Add a FREE HuggingFace token (takes 2 minutes):
   1. Get token: https://huggingface.co/settings/tokens
   2. In Streamlit Cloud: Settings ‚Üí Secrets, add:
      HF_TOKEN = "your_token_here"
   3. Reboot the app

üí° With a token you get:
   ‚úÖ Priority access to AI models
   ‚úÖ No rate limits  
   ‚úÖ Faster responses
   ‚úÖ 100% FREE!"""

# Initialize session state
if 'initialized' not in st.session_state:
    with st.spinner('üöÄ Initializing RAG System...'):
        st.session_state.model, st.session_state.collection = initialize_system()
        st.session_state.initialized = True

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Header
st.markdown('<div class="main-header">ü§ñ RAG System</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">Intelligent Question Answering - Cloud Deployment</div>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.header("üìä System Status")
    
    # Get HuggingFace token from secrets or input
    if 'hf_token' not in st.session_state:
        try:
            # Try to read from Streamlit secrets (both syntaxes)
            if "HF_TOKEN" in st.secrets:
                st.session_state.hf_token = st.secrets["HF_TOKEN"]
            else:
                st.session_state.hf_token = ""
        except Exception as e:
            st.session_state.hf_token = ""
    
    # Show token status
    if st.session_state.hf_token:
        st.success("‚úÖ HuggingFace token configured!")
    else:
        with st.expander("üîë HuggingFace Token Required", expanded=True):
            st.warning("‚ö†Ô∏è Token not found in secrets. Add it for AI models to work!")
            st.info("""
            **How to add token:**
            1. Get token: https://huggingface.co/settings/tokens
            2. In Streamlit Cloud: ‚öôÔ∏è Settings ‚Üí Secrets
            3. Add this line:
               ```
               HF_TOKEN = "your_token_here"
               ```
            4. Save & Reboot app
            
            **Or enter temporarily below (won't persist):**
            """)
            hf_input = st.text_input("Token:", type="password", key="hf_input")
            if st.button("Save Token (This Session Only)"):
                st.session_state.hf_token = hf_input
                st.success("‚úÖ Token saved for this session!")
                st.rerun()
    
    # Get statistics
    try:
        count = st.session_state.collection.count()
    except:
        count = 0
    
    # Display stats
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"""
        <div class="stat-card">
            <div class="stat-number">{count}</div>
            <div class="stat-label">Documents</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="stat-card">
            <div class="stat-number">FREE</div>
            <div class="stat-label">Cost</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    st.markdown("### üîß System Info")
    st.info("""
    **Vector Store:** ChromaDB  
    **Embedding Model:** MiniLM-L6-v2  
    **LLM:** HuggingFace Mistral-7B  
    **Deployment:** Streamlit Cloud ‚òÅÔ∏è
    """)
    
    st.markdown("---")
    
    # PDF Upload section
    st.markdown("### üìÑ Upload Document")
    uploaded_file = st.file_uploader(
        "Choose a PDF or TXT file",
        type=['pdf', 'txt'],
        help="Upload a document to add to your knowledge base"
    )
    
    if uploaded_file is not None:
        if st.button("üöÄ Process Document", type="primary", use_container_width=True):
            with st.spinner("üìñ Processing document..."):
                try:
                    import tempfile
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name
                    
                    # Process document
                    from document_processor import DocumentProcessor
                    
                    processor = DocumentProcessor()
                    chunks = processor.process_document(tmp_path)
                    
                    if chunks:
                        # Add to vector store
                        embeddings = st.session_state.model.encode([chunk['text'] for chunk in chunks])
                        
                        st.session_state.collection.add(
                            documents=[chunk['text'] for chunk in chunks],
                            embeddings=embeddings.tolist(),
                            metadatas=[{
                                'source': uploaded_file.name,
                                'chunk_id': i
                            } for i in range(len(chunks))],
                            ids=[f"upload_{int(time.time())}_{i}" for i in range(len(chunks))]
                        )
                        
                        st.success(f"‚úÖ Processed {uploaded_file.name}!")
                        st.info(f"üìä Added {len(chunks)} chunks")
                        
                        os.unlink(tmp_path)
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("‚ùå Could not extract text")
                        os.unlink(tmp_path)
                        
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
    
    if st.button("üîÑ Clear Chat History"):
        st.session_state.chat_history = []
        st.rerun()

# Main content
st.markdown("### üí¨ Ask me anything!")

question = st.text_input(
    "Your question:",
    placeholder="e.g., What is machine learning?",
    key="question_input"
)

col1, col2 = st.columns([1, 4])

with col1:
    ask_button = st.button("üöÄ Ask", type="primary", use_container_width=True)

# Process question
if ask_button and question:
    with st.spinner('ü§î Thinking...'):
        # Search for relevant documents
        query_emb = st.session_state.model.encode([question])
        results = st.session_state.collection.query(
            query_embeddings=query_emb.tolist(),
            n_results=3
        )
        
        # Build context
        context = "\n\n".join(results['documents'][0]) if results['documents'][0] else "No context available"
        
        # Try to generate answer with LLM first, fallback to local RAG
        answer = None
        use_local_rag = True  # Default to local RAG
        
        if st.session_state.hf_token:
            # Generate answer with LLM
            prompt = f"""Based on the following context, answer the question. If the context doesn't contain relevant information, say so.

Context:
{context}

Question: {question}

Answer:"""
            
            answer = query_huggingface(prompt, st.session_state.hf_token)
            
            # Check if answer is an error message - if so, fall back to local RAG
            if answer and not (answer.startswith("‚ö†Ô∏è") or answer.startswith("‚ùå") or "unavailable" in answer.lower()):
                use_local_rag = False
        
        # Use local RAG-based answer if no valid LLM response
        if use_local_rag:
            answer = generate_answer_from_context(question, results['documents'][0] if results['documents'] else [])
        
        # Add to chat history
        st.session_state.chat_history.append({
            'question': question,
            'answer': answer,
            'sources': results['documents'][0][:3] if results['documents'] else []
        })

# Display chat history
if st.session_state.chat_history:
    st.markdown("---")
    
    for chat in reversed(st.session_state.chat_history):
        with st.container():
            st.markdown(f"**üôã You asked:** {chat['question']}")
            
            st.markdown(f"""
            <div class="answer-box">
                <strong>ü§ñ Answer:</strong><br>
                {chat['answer']}
            </div>
            """, unsafe_allow_html=True)
            
            if chat['sources']:
                with st.expander(f"üìö View {len(chat['sources'])} source(s)"):
                    for j, source in enumerate(chat['sources'], 1):
                        st.markdown(f"""
                        <div class="source-box">
                            <strong>Source {j}:</strong><br>
                            {source}
                        </div>
                        """, unsafe_allow_html=True)
            
            st.markdown("---")
else:
    st.info("""
    üëã **Welcome to the RAG System!**
    
    Upload a PDF document using the sidebar, then ask questions about it!
    
    Just type your question above and click "Ask" üöÄ
    """)

# Footer
st.markdown("---")
st.markdown("""
<div class="footer-text">
    ü§ñ RAG System | Deployed on Streamlit Cloud | 100% FREE ‚òÅÔ∏è
</div>
""", unsafe_allow_html=True)

